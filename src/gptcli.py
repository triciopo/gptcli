#!/usr/bin/env python3
"""
gptcli: Run OpenAI's ChatGPT from the command line

This module provides the main logic to
generate responses to prompts using OpenAI's API.
"""

import argparse
import configparser
import os
import readline  # noqa: F401
import signal
import sys
from pathlib import Path

import openai
from appdirs import user_config_dir
from rich import print as rprint

API_ENDPOINT = "https://api.openai.com/v1/chat/completions"
MODELS_LIST = ["gpt-3.5-turbo", "gpt-4", "gpt-4-32k"]


def post(api_key, prompt, model, num_completions, temperature):
    """
    Sends a prompt to the OpenAI API and retrieves one or more completions.

    Arguments:
        api_key (str): The user's OpenAI API key.
        prompt (str): The prompt to generate a response to.
        model (str): The model to use for generating the response.
        num_completions (int): Number of responses to generate.
        temperature (float): What sampling temperature to use.

    Returns:
        A dictionary containing the completion(s) generated by the API.
    """
    stream = num_completions == 1
    try:
        response = openai.ChatCompletion.create(
            messages=[{"role": "user", "content": prompt}],
            model=model,
            temperature=temperature,
            api_key=api_key,
            n=num_completions,
            stream=stream,
            frequency_penalty=0,
            presence_penalty=0,
            top_p=1,
        )
        return response

    except (openai.error.APIError, openai.error.InvalidRequestError) as error:
        rprint(f"[bold red]ERROR: [/]{error}")
        sys.exit(1)
    except openai.error.AuthenticationError:
        rprint(f"[bold red]ERROR: [/]Invalid OpenAI API key.")
        sys.exit(1)


def arg_parser():
    """
    This function creates an argparse parser
    that allows users to input arguments and options.

    Arguments:
        None

    Returns:
        args (Namespace): A Namespace object containing
        the parsed arguments and their values.
    """
    parser = argparse.ArgumentParser(
        description="Run OpenAI's ChatGPT from the commandline"
    )
    parser.add_argument(
        "prompt",
        type=str,
        nargs=argparse.REMAINDER,
        default=argparse.SUPPRESS,
        help="Main prompt (Leave blank to enable chat mode)",
    )
    parser.add_argument(
        "-m",
        "--model",
        type=str,
        nargs=1,
        help="Select model (default: gpt-3.5-turbo)",
    )
    parser.add_argument(
        "-f",
        "--file",
        type=str,
        nargs=1,
        help="Specify a file to include in the prompt",
    )
    parser.add_argument(
        "-n",
        "--num",
        type=int,
        help="Number of responses generated by one input (default: 1)",
    )
    parser.add_argument(
        "-t",
        "--temp",
        type=float,
        help="What sampling temperature to use, between 0 and 2. (default: 1)",
    )
    config = get_config()

    # Add predefined prompts from the config file
    if config.has_section("args_prompts"):
        for key in config["args_prompts"]:
            cleaned = "".join(filter(str.isalnum, key)).lower()
            key_prompt = config["args_prompts"][key]
            parser.add_argument(
                f"--{cleaned}",
                action="store_true",
                help=f"Prompt: {key_prompt[:45]} \
                     {('...' if len(key_prompt) > 45 else '')}",
            )
    return parser.parse_args()


def get_config():
    """
    Returns the configuration values for gptcli.
    If the configuration file does not exist, creates it with default values.

    Arguments:
        None

    Returns:
        ConfigParser: An instance of the ConfigParser class containing
        the configuration values for gptcli.
    """
    # Get the path to the configuration file and create a new ConfigParser object
    config_dir = user_config_dir("gptcli")
    config_file = f"{config_dir}/config.ini"
    config_path = Path(config_file)
    config = configparser.ConfigParser(strict=False)

    # If the configuration file doesn't exist, create it with default values
    if not config_path.is_file():
        config["api"] = {"api_key": "<INSERT API KEY HERE>", "model": "gpt-3.5-turbo"}
        config["file_format_prompt"] = {
            ".txt": "Summarize this for me:",
        }
        config["args_prompts"] = {
            "code": "Answer only with code. Do not include comments or explanation.",
        }
        if not config_path.exists():
            Path(config_dir).mkdir(parents=True, exist_ok=True)
        with open(config_file, "w", encoding="utf-8") as configfile:
            config.write(configfile)

    config.read(config_file)
    return config


def get_prompt_from_file(prompt, file, file_prompt, config, args):
    """
    This function reads a file and generates a prompt based on its contents.

    Arguments:
    prompt (str): A string containing a prompt.
    file (str): A string containing the path to the file.
    files_prompt (dict): A dictionary containing the prompt based on file extension.
    config (ConfigParser): A ConfigParser object that stores the configuration settings.
    args (argparse.Namespace): Instance of the Namespace class containing the args.

    Returns:
    prompt (str): A string containing the prompt.
    """
    if not Path(file).is_file():
        print(f"Error: file {file} does not exist.")
        sys.exit(1)

    if prompt is None:
        prompt = ""
        file_format = Path(file).suffix
        if get_pre_prompt(config, args) is not None:
            # Prioritize args instead of file format
            with open(file, "r", encoding="utf-8") as file_content:
                prompt = f"{file_content.read()}"

        elif file_format in file_prompt:
            main_prompt = file_prompt.get(file_format)
            with open(file, "r", encoding="utf-8") as file_content:
                prompt = f"{main_prompt}\n{file_content.read()}"

        else:
            print("Please specify a prompt when using -f.")
            print(
                "Alternatively, create a predefined prompt "
                "based on the file extension in the config file."
            )
            sys.exit(1)
    else:
        with open(file, "r", encoding="utf-8") as file_content:
            prompt = f"\n{file_content.read()}"
    return prompt


def get_pre_prompt(config, args):
    """
    Gets prompts from the config based on supplied arguments.

    Args:
        config (ConfigParser): The configuration values for gptcli.
        args (argparse.Namespace): The parsed arguments.

    Returns:
        str: The concatenated prompt lines or None if no prompts were found
    """
    prompt = []
    if config.has_section("args_prompts"):
        for key in config["args_prompts"]:
            if key is not None:
                key_cleaned = "".join(filter(str.isalnum, key)).lower()
                arg_value = getattr(args, key_cleaned)
                if arg_value:
                    prompt.append(config["args_prompts"][key_cleaned])
    return "\n".join(prompt) if len(prompt) > 0 else None


def stream_output(api_key, prompt, model, num_completions, temperature):
    """
    Formats and prints the output based on number of completions.
    Arguments:
        api_key (str): OpenAI API key.
        prompt (str): The main prompt.
        model (str): Name of the model to use.
        num_completions (int): Number of completions to generate.
    Returns:
        None
    """
    if num_completions == 1:
        output = post(api_key, prompt, model, num_completions, temperature)
        for chunk in output:
            if chunk:
                content = chunk["choices"][0].get("delta", {}).get("content")
                if content is not None:
                    print(content, end="", flush=True)
        print()
    else:
        choices = post(api_key, prompt, model, num_completions, temperature).get(
            "choices", []
        )
        output = [c.get("message", {}).get("content", "") for c in choices if c]
        print("\n\n".join(output))


def chat_mode(api_key, model, num_completions, temperature):
    """
    Enters chat mode and prompts the user for input.

    Arguments:
        api_key (str): OpenAI API key.
        model (str): Name of the model to use.
        num_completions (int): Number of completions to generate.
        temperature (float): What sampling temperature to use.

    Returns:
        None
    """

    print("Starting chat mode")
    print("Press CTRL+C to stop at anytime")

    # Start chat loop
    while True:
        prompt = input(f"\n>>> ")
        stream_output(api_key, prompt, model, num_completions, temperature)


def signal_handler(signal, frame):
    """Exit on CTRL+C without printing errors"""
    print("")
    sys.exit(0)


def main():
    signal.signal(signal.SIGINT, signal_handler)  # Allow CTRL+C without traceback
    # Get arguments and config
    args = arg_parser()
    config = get_config()

    # Get model and file extension prompt from the config file.
    model = config.get("api", "model")
    files_prompt = config["file_format_prompt"]

    # Get model, number of completions, temperature, file and prompt specified on args
    num_completions = max(args.num, 1) if args.num else 1
    temperature = min(max(args.temp, 0), 2) if args.temp else 1
    model_name = args.model[0] if args.model else model
    file = args.file[0] if args.file else None
    prompt = " ".join(args.prompt) if args.prompt else None
    pre_prompt = get_pre_prompt(config, args)

    # Get api key
    try:
        api_key = os.environ["OPENAI_API_KEY"]
    except KeyError:
        api_key = config.get("api", "api_key")

    if model_name not in MODELS_LIST:  # gpt-3.5-turbo, gpt-4, gpt-4-32k
        rprint(f"[bold red]ERROR: [/]Invalid model.")
        rprint(f"Available models: {', '.join(MODELS_LIST)}")
        sys.exit(1)

    if pre_prompt is not None:  # prompt from config file
        prompt = f"{pre_prompt}{chr(10) + prompt if prompt is not None else ''}"

    if file is not None:  # File specified
        if prompt is None:  # File specified but no prompt
            prompt = get_prompt_from_file(prompt, file, files_prompt, config, args)
        else:
            prompt += get_prompt_from_file(prompt, file, files_prompt, config, args)

    if prompt is None:  # no prompt specified
        chat_mode(api_key, model_name, num_completions, temperature)

    stream_output(api_key, prompt, model_name, num_completions, temperature)


if __name__ == "__main__":
    main()
